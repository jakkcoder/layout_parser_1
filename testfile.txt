from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from google.cloud import bigquery

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2023, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG('bigquery_slot_usage_check',
          default_args=default_args,
          description='Check user emails and slot usage in BigQuery',
          schedule_interval=timedelta(days=7),  # Runs once every week
          catchup=False)

def check_user_slot_usage():
    client = bigquery.Client()
    one_week_ago = datetime.utcnow() - timedelta(days=7)
    query = f"""
    SELECT user_email, SUM(slot_ms) AS total_slot_ms
    FROM `region-us`.INFORMATION_SCHEMA.JOBS_BY_USER
    WHERE creation_time BETWEEN TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY) AND CURRENT_TIMESTAMP()
    GROUP BY user_email
    """
    query_job = client.query(query)
    results = query_job.result()
    
    for row in results:
        print(f"User: {row.user_email}, Total Slots Used: {row.total_slot_ms}")

check_usage = PythonOperator(
    task_id='check_user_slot_usage',
    python_callable=check_user_slot_usage,
    dag=dag
)
