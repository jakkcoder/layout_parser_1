
from google.cloud import bigquery

client = bigquery.Client()

dataset_id = 'your_dataset_id'
table_id = 'bw_data_test'

schema = [
    bigquery.SchemaField("SpendHistory", "RECORD", mode="REPEATED", fields=[
        bigquery.SchemaField("D", "INTEGER", mode="NULLABLE"),
        bigquery.SchemaField("S", "INTEGER", mode="NULLABLE"),
        bigquery.SchemaField("IsDB", "STRING", mode="NULLABLE"),
        bigquery.SchemaField("Spend", "INTEGER", mode="NULLABLE"),
        bigquery.SchemaField("Paths", "RECORD", mode="REPEATED", fields=[
            # Define the schema for Paths here
        ])
    ]),
    bigquery.SchemaField("Meta", "RECORD", mode="NULLABLE", fields=[
        # Define the schema for Meta here
    ]),
    bigquery.SchemaField("Attributes", "RECORD", mode="NULLABLE", fields=[
        # Define the schema for Attributes here
    ]),
    bigquery.SchemaField("FirstIndexed", "INTEGER", mode="NULLABLE"),
    bigquery.SchemaField("LastIndexed", "INTEGER", mode="NULLABLE"),
    bigquery.SchemaField("Lookup", "STRING", mode="NULLABLE"),
    bigquery.SchemaField("SalesRevenue", "INTEGER", mode="NULLABLE")
]

table_ref = client.dataset(dataset_id).table(table_id)
table = bigquery.Table(table_ref, schema=schema)
table = client.create_table(table)

# Load the JSON data
with open('path_to_your_file.json', 'rb') as source_file:
    job = client.load_table_from_file(source_file, table_ref, job_config=bigquery.LoadJobConfig(source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON))

job.result()  # Wait for the job to complete

print("Loaded {} rows into {}:{}.".format(job.output_rows, dataset_id, table_id))
