from google.cloud import bigquery

# Initialize a BigQuery client
client = bigquery.Client()

# Define your dataset and table names
dataset_id = 'your_dataset_id'
table_id = 'your_table_id'

# Load the schema from the JSON file
with open('nested_schema.json', 'r') as f:
    schema = json.load(f)

# Convert the JSON schema to BigQuery SchemaField objects
def json_to_bq_schema(json_schema):
    return [
        bigquery.SchemaField(
            field['name'],
            field['type'],
            mode=field.get('mode', 'NULLABLE'),
            fields=json_to_bq_schema(field['fields']) if field.get('fields') else []
        ) for field in json_schema
    ]

bq_schema = json_to_bq_schema(schema)

# Define the table reference
table_ref = client.dataset(dataset_id).table(table_id)

# Create a table with the schema
table = bigquery.Table(table_ref, schema=bq_schema)
table = client.create_table(table)  # API request

print(f"Created table {table.project}.{table.dataset_id}.{table.table_id}")

# Load data into the table (example with a JSON file)
uri = 'gs://your-bucket/path/to/your-file.json'

job_config = bigquery.LoadJobConfig(
    schema=bq_schema,
    source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,
)

load_job = client.load_table_from_uri(
    uri,
    table_ref,
    job_config=job_config,
)  # Make an API request.

load_job.result()  # Wait for the job to complete.

print(f"Loaded {load_job.output_rows} rows into {dataset_id}:{table_id}.")
