from google.cloud import bigquery
import json

# Define your project ID and dataset/table names
project_id = 'your_project_id'
dataset_id = 'your_dataset_id'
table_id = 'your_table_id'
table_full_id = f"{project_id}.{dataset_id}.{table_id}"

# Load the schema from JSON file
with open('path/to/your/nested_schema.json') as schema_file:
    schema_json = json.load(schema_file)

# Convert schema JSON to BigQuery SchemaField list
schema = [bigquery.SchemaField.from_api_repr(field) for field in schema_json]

# Initialize a BigQuery client
client = bigquery.Client(project=project_id)

# Create a table if it doesn't exist
table = bigquery.Table(table_full_id, schema=schema)
table = client.create_table(table, exists_ok=True)  # Make an API request.

# Load DataFrame to BigQuery
job = client.load_table_from_dataframe(df, table_full_id)

# Wait for the job to complete
job.result()

print(f"Loaded {job.output_rows} rows into {table_full_id}.")
